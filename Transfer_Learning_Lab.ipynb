{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning with VGG, Inception and ResNet\n",
    "\n",
    "In this lab, you will continue exploring transfer learning. You've already explored feature extraction with Alexnet and TensorFlow. Next, you will use Keras to explore feature extraction with the VGG, Inception and ResNet architectures. The models you will use were trained for days or weeks on the [ImageNet dataset](http://www.image-net.org/). Thus, the weights escapsulate higher-level features learned from thousands of classes.\n",
    "\n",
    "We'll use two datasets in this lab:\n",
    "\n",
    "1. [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset)\n",
    "2. [Cifar10](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "\n",
    "How will the pretrained model perform on the new datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten, Input, Dropout\n",
    "from keras.models import Sequential\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_bottleneck_data(network, dataset):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        network - String, one of 'resnet', 'vgg', 'inception'\n",
    "        dataset - String, one of 'cifar10', 'traffic'\n",
    "    \"\"\"\n",
    "    train_file = '{}_{}_bottleneck_features_train.p'.format(network, dataset)\n",
    "    validation_file = '{}_{}_bottleneck_features_validation.p'.format(network, dataset)\n",
    "        \n",
    "    with open(train_file, 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    with open(validation_file, 'rb') as f:\n",
    "        validation_data = pickle.load(f)\n",
    "        \n",
    "    X_train = train_data['features']\n",
    "    y_train = train_data['labels']\n",
    "    X_val = validation_data['features']\n",
    "    y_val = validation_data['labels']\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Due to the size of the VGG, Inception and ResNet feature extraction takes a non-trivial amount of time even on a decent GPU. To compensate for this and allow you to get a feel for feature extraction even if you just have a modest CPU we precomputed **bottleneck features** from the networks for you to use. \n",
    "\n",
    "The bottleneck features were computed by running one pass of the network on the training and validation, the outputs (features) from this pass are then saved. These are the bottleneck features! Because the base network weights are frozen during, the output is the exact same for an image on each run. If we cache the output we avoid the need to feed the image through the entire base network again.\n",
    "\n",
    "Before you try feature extraction on pretrained models it's a good idea to take a moment and run the classifier you used in the Traffic Sign project on the Cifar10 dataset. Cifar10 images are also (32, 32, 3) so the only thing you'll need to change is the number of classes to 10 instead of 43.\n",
    "\n",
    "Cool, now you have something to compare the Cifar10 feature extraction results with!\n",
    "\n",
    "Keep in mind the following as you experiment:\n",
    "\n",
    "_Does feature extraction outperform the Traffic Signs classifier on the Cifar10 dataset? Why?_\n",
    "\n",
    "_Does feature extraction outperform the Traffic Signs classifier on the Traffic Signs dataset? Why?_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load bottleneck data\n",
    "X_train, y_train, X_val, y_val = load_bottleneck_data('resnet', 'traffic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "nb_epoch = 50\n",
    "batch_size = 32\n",
    "nb_classes = 43 # NOTE: change this to 43 if using traffic sign data\n",
    "\n",
    "print('Feature shape', X_train.shape[1:])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=X_train.shape[1:]))\n",
    "\n",
    "# TODO: Define the rest of your network here\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    nb_epoch=nb_epoch, \n",
    "    batch_size=batch_size, \n",
    "    validation_data=(X_val, y_val), \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "By now you should have a good feel for feature extraction and when it might be a good choice. To end this lab, let's summarize when we should consider:\n",
    "\n",
    "1. Feature extraction (train only the top-level of the network, the rest of the network remains fixed)\n",
    "2. Finetuning (train the entire network end-to-end, start with pretrained weights)\n",
    "3. Training from scratch (train the entire network end-to-end, start from random weights)\n",
    "\n",
    "**Consider feature extraction when ...**\n",
    "\n",
    "If dataset is small and similar to the original dataset. The higher-level features learned from the original dataset should be relevant to the new dataset.\n",
    "\n",
    "**Consider finetuning when ...** \n",
    "\n",
    "If the dataset is large and similar to the original dataset. In this case we should be much more confident we won't overfit so it should be safe to alter the original weights.\n",
    "\n",
    "If the dataset is small and very different from the original dataset. You could also make the case for training from scratch. If we choose to finetune it might be a good idea to only use features found earlier on in the network, features found later might be too dataset specific.\n",
    "\n",
    "**Consider training from scratch when ...**\n",
    "\n",
    "If the dataset is large and very different from the original dataset. In this case we have enough data to confidently train from scratch. However, even in this case it might be more beneficial to finetune and the entire network from pretrained weights.\n",
    "\n",
    "---\n",
    "\n",
    "Most importantly, keep in mind for a lot of problems you won't need an architecture as complicated and powerful as VGG, Inception, or ResNet. These architectures were made for the task of classifying thousands of complex classes. A much smaller network might be a much better fit for your problem, especially if you can comfortably train it on moderate hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
