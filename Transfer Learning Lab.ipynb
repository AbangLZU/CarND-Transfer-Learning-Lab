{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning with VGG, Inception and ResNet\n",
    "\n",
    "In this lab, you will continue exploring transfer learning. You've already explored feature extraction with Alexnet and TensorFlow. Next, you will use Keras to explore feature extraction with the VGG, Inception and ResNet architectures. The models you will use were trained for days or weeks on the [ImageNet dataset](http://www.image-net.org/). Thus, the weights escapsulate higher-level features learned from thousands of classes.\n",
    "\n",
    "We'll use two datasets in this lab:\n",
    "\n",
    "1. [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset)\n",
    "2. [Cifar10](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "\n",
    "How will the pretrained model perform on the new datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, Flatten, Input, AveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "from skimage.transform import resize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data functions\n",
    "traffic_training_file = 'data/train.p'\n",
    "traffic_testing_file = 'data/test.p'\n",
    "\n",
    "def load_traffic():\n",
    "    with open(traffic_training_file, mode='rb') as f:\n",
    "        train = pickle.load(f)\n",
    "    with open(traffic_testing_file, mode='rb') as f:\n",
    "        test = pickle.load(f)\n",
    "    return train['features'], train['labels'], test['features'], test['labels']\n",
    "\n",
    "\n",
    "# NOTE: it will take a while on first use since Keras will download the dataset\n",
    "def load_cifar10():\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Resizing all at once will take up too much data, so we resize it per batch\n",
    "# VGG, Inception, and ResNet expect a (224, 224, 3) input\n",
    "def gen_and_resize_data(data, labels, batch_size, size=(224, 224)):\n",
    "    def _f():\n",
    "        start = 0\n",
    "        end = start + batch_size\n",
    "        n = data.shape[0]\n",
    "        while True:\n",
    "            X_batch_old, y_batch = data[start:end], labels[start:end]\n",
    "            X_batch = []\n",
    "            for i in range(X_batch_old.shape[0]):\n",
    "                img = resize(X_batch_old[i, ...], size)\n",
    "                X_batch.append(img)\n",
    "\n",
    "            X_batch = np.array(X_batch)\n",
    "            start += batch_size\n",
    "            end += batch_size\n",
    "            if start >= n:\n",
    "                start = 0\n",
    "                end = batch_size\n",
    "\n",
    "            yield (X_batch, y_batch)\n",
    "    return _f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Before you try feature extraction on pretrained models it's a good idea to take a moment and run the classifier you used in the Traffic Sign project on the Cifar10 dataset. Cifar10 images are also (32, 32, 3) so the only thing you'll need to change is the number of classes to 10 instead of 43.\n",
    "\n",
    "Cool, now you have something to compare the Cifar10 feature extraction results with!\n",
    "\n",
    "Keep in mind the following as you experiment:\n",
    "\n",
    "_Does feature extraction outperform the Traffic Signs classifier on the Cifar10 dataset? Why?_\n",
    "\n",
    "_Does feature extraction outperform the Traffic Signs classifier on the Traffic Signs dataset? Why?_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load and preprocess data\n",
    "X_train, y_train, X_test, y_test = load_cifar10()\n",
    "# X_train, y_train, X_test, y_test = load_traffic()\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# 0-255 -> 0-1\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "nb_epochs = 5\n",
    "batch_size = 32\n",
    "nb_classes = 10 # NOTE: change this to 43 if using traffic sign data\n",
    "\n",
    "# define model\n",
    "input_tensor = Input(shape=(224, 224, 3))\n",
    "\n",
    "# NOTE: It will take a while on the first use since Keras will download the weights for the model\n",
    "\n",
    "# pretrained_model = VGG16(input_tensor=input_tensor, include_top=False, weights='imagenet')\n",
    "# x = pretrained_model.layers[3].output # after first maxpool\n",
    "\n",
    "# pretrained_model = InceptionV3(input_tensor=input_tensor, include_top=False, weights='imagenet')\n",
    "# x = pretrained_model.layers[28].output # after first inception block merge\n",
    "\n",
    "pretrained_model = ResNet50(input_tensor=input_tensor, include_top=False, weights='imagenet')\n",
    "# x = pretrained_model.layers[17].output # after first conv block merge\n",
    "x = pretrained_model.output\n",
    "\n",
    "# top layers\n",
    "# if you use VGG or an output that's not the last output in the other models\n",
    "# uncomment this AveragePooling2D. It prevents this Dense layer for being\n",
    "# super massive\n",
    "# x = AveragePooling2D((7,7))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dense(nb_classes, activation='softmax')(x)\n",
    "model = Model(pretrained_model.input, x)\n",
    "\n",
    "# freeze pretrained model layers\n",
    "for layer in pretrained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can use this to see where to change the output of the pretrained model\n",
    "for i, l in enumerate(model.layers):\n",
    "    print(i, l.trainable, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "train_gen = gen_and_resize_data(X_train, y_train, batch_size)\n",
    "test_gen = gen_and_resize_data(X_test, y_test, batch_size)\n",
    "model.fit_generator(\n",
    "    train_gen(),\n",
    "    X_train.shape[0],\n",
    "    nb_epochs,\n",
    "    nb_val_samples=X_test.shape[0],\n",
    "    validation_data=test_gen())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "By now you should have a good feel for feature extraction and when it might be a good choice. To end this lab, let's summarize when we should consider:\n",
    "\n",
    "1. Feature extraction (train only the top-level of the network, the rest of the network remains fixed)\n",
    "2. Finetuning (train the entire network end-to-end, start with pretrained weights)\n",
    "3. Training from scratch (train the entire network end-to-end, start from random weights)\n",
    "\n",
    "**Consider feature extraction when ...**\n",
    "\n",
    "If dataset is small and similar to the original dataset. The higher-level features learned from the original dataset should be relevant to the new dataset.\n",
    "\n",
    "**Consider finetuning when ...** \n",
    "\n",
    "If the dataset is large and similar to the original dataset. In this case we should be much more confident we won't overfit so it should be safe to alter the original weights.\n",
    "\n",
    "If the dataset is small and very different from the original dataset. You could also make the case for training from scratch. If we choose to finetune it might be a good idea to only use features found earlier on in the network, features found later might be too dataset specific.\n",
    "\n",
    "**Consider training from scratch when ...**\n",
    "\n",
    "If the dataset is large and very different from the original dataset. In this case we have enough data to confidently train from scratch. However, even in this case it might be more beneficial to finetune and the entire network from pretrained weights.\n",
    "\n",
    "---\n",
    "\n",
    "Most importantly, keep in mind for a lot of problems you won't need an architecture as complicated and powerful as VGG, Inception, or ResNet. These architectures were made for the task of classifying thousands of complex classes. A much smaller network might be a much better fit for your problem, especially if you can comfortably train it on moderate hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
